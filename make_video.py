import os
import json
import pysrt
import re
from moviepy.editor import *
from moviepy.config import change_settings

# üëá G·ªçi file animation ch·ª©a hi·ªáu ·ª©ng hi·ªán ƒë·∫°i
from animation import apply_animation 

# C·∫•u h√¨nh ImageMagick
change_settings({"IMAGEMAGICK_BINARY": r"C:\Program Files\ImageMagick-7.1.2-Q16-HDRI\magick.exe"})

# --- C·∫§U H√åNH ---
PATHS = {
    'audio_file': 'data/input/tc2.mp3',
    'image_srt_file': 'images.srt',
    'sentences_srt_file': 'sentences.srt', 
    'output_video': 'data/video_output/FINAL_VIDEO.mp4',
    'hand_icon': 'data/input/hand.png' # ƒê∆∞·ªùng d·∫´n ·∫£nh c√°i tay (n·∫øu d√πng hi·ªáu ·ª©ng draw)
}

SETTINGS = {
    'anim_duration': 0.8, # TƒÉng th·ªùi gian animation l√™n x√≠u cho m∆∞·ª£t
    'bg_color': (255, 255, 255),
    'video_fps': 24,
    'default_size': (1366, 768),
    'font': 'Arial-Bold',
    'fontsize': 42,
    'font_color': 'yellow',
    'stroke_color': 'black',
    'stroke_width': 1.8,
    'sub_bottom_margin': 60
}

# H√†m ti·ªán √≠ch: ƒê·∫£m b·∫£o s·ªë lu√¥n ch·∫µn
def make_even(n):
    return int(n) if int(n) % 2 == 0 else int(n) + 1

def create_balanced_subtitle(txt, start, end, v_width, v_height):
    txt_clip = TextClip(txt, font=SETTINGS['font'], fontsize=SETTINGS['fontsize'],
                        color=SETTINGS['font_color'], stroke_color=SETTINGS['stroke_color'],
                        stroke_width=SETTINGS['stroke_width'], method='caption', align='Center',
                        size=(v_width * 0.85, None))
    y_pos = v_height - txt_clip.h - SETTINGS['sub_bottom_margin']
    return txt_clip.set_start(start).set_end(end).set_position(('center', y_pos))

# --- H√ÄM RENDER CH√çNH ---
def render_video():
    print("\nüéûÔ∏è --- ƒêANG T·ªîNG H·ª¢P VIDEO ---")
    
    # 0. CHU·∫®N B·ªä
    audio_clip = AudioFileClip(PATHS['audio_file'])
    total_duration = audio_clip.duration + 1.0
    
    # üëá KH·ªûI T·∫†O BI·∫æN QUAN TR·ªåNG (Fix l·ªói NameError)
    layout_caches = {} 
    video_size = SETTINGS['default_size']
    
    # 1. ƒê·ªåC V√Ä S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG SRT
# 1. ƒê·ªåC V√Ä S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG SRT
    if not os.path.exists(PATHS['image_srt_file']):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {PATHS['image_srt_file']}")
        return

    with open(PATHS['image_srt_file'], 'r', encoding='utf-8') as f:
        content = f.read().strip()

    # --- B·ªî SUNG LOGIC X·ª¨ L√ù ƒê·ªäNH D·∫†NG M·ªòT D√íNG ---
    # Ki·ªÉm tra xem c√≥ ph·∫£i ƒë·ªãnh d·∫°ng: [ID] [Time] [Text] tr√™n c√πng 1 d√≤ng kh√¥ng
    # V√≠ d·ª•: 1 00:00:00,031 --> 00:00:11,227 data/output/1/5.png | pop
    line_format_pattern = r'^(\d+)\s+(\d{2}:\d{2}:\d{2},\d{3}\s+-->\s+\d{2}:\d{2}:\d{2},\d{3})\s+(.+)$'
    
    lines = content.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if not line: continue
        
        match = re.match(line_format_pattern, line)
        if match:
            # N·∫øu kh·ªõp ƒë·ªãnh d·∫°ng m·ªôt d√≤ng, t√°ch ra l√†m 3 d√≤ng chu·∫©n SRT
            fixed_lines.append(match.group(1)) # ID
            fixed_lines.append(match.group(2)) # Time
            fixed_lines.append(match.group(3)) # Text
            fixed_lines.append("")             # D√≤ng tr·ªëng ph√¢n c√°ch
        else:
            # N·∫øu kh√¥ng kh·ªõp, c√≥ th·ªÉ n√≥ ƒë√£ l√† ƒë·ªãnh d·∫°ng chu·∫©n ho·∫∑c ƒë·ªãnh d·∫°ng l·ªói kh√°c
            fixed_lines.append(line)

    # G·ªôp l·∫°i th√†nh n·ªôi dung SRT chu·∫©n
    final_content = "\n".join(fixed_lines)
    
    # S·ª≠ d·ª•ng pysrt ƒë·ªÉ parse n·ªôi dung ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
    raw_subs = pysrt.from_string(final_content)
    scenes = [] 
    current_folder = None
    current_scene_items = []

    # Gom nh√≥m c√°c ·∫£nh theo folder (Scene)
    for sub in raw_subs:
        content = sub.text.strip().split('|')
        path_raw = content[0].strip()
        effect_type = content[1].strip().lower() if len(content) > 1 else 'pop'
        
        if not os.path.exists(path_raw): continue
        
        sub.img_path = path_raw
        sub.anim_effect = effect_type 
        
        folder = os.path.dirname(path_raw)
        if folder != current_folder:
            if current_folder is not None:
                scenes.append({'folder': current_folder, 'items': current_scene_items})
            current_folder = folder
            current_scene_items = []
        current_scene_items.append(sub)
    
    if current_folder is not None:
        scenes.append({'folder': current_folder, 'items': current_scene_items})

    # 2. X·ª¨ L√ù ·∫¢NH & ANIMATION
    image_layers = []
    
    for i, scene in enumerate(scenes):
        folder_path = scene['folder']
        items = scene['items']
        
        # T√≠nh th·ªùi gian k·∫øt th√∫c c·ªßa Scene
        if i < len(scenes) - 1:
            next_scene_start = scenes[i+1]['items'][0].start.ordinal / 1000.0
            scene_end_time = next_scene_start
        else:
            scene_end_time = total_duration

        # Load Layout JSON
        if folder_path not in layout_caches:
            json_path = os.path.join(folder_path, 'layout.json')
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    w = make_even(data['original_size']['w'])
                    h = make_even(data['original_size']['h'])
                    layout_caches[folder_path] = {
                        'icons': {item['file']: item for item in data['icons']},
                        'size': (w, h)
                    }
                    video_size = layout_caches[folder_path]['size']
            else:
                layout_caches[folder_path] = None
        
        layout_data = layout_caches.get(folder_path)

        for item in items:
            filename = os.path.basename(item.img_path)
            start_time = item.start.ordinal / 1000.0
            duration = scene_end_time - start_time
            if duration <= 0: duration = 0.5 

            if layout_data and filename in layout_data['icons']:
                info = layout_data['icons'][filename]
                
                # T·∫°o clip ·∫£nh g·ªëc
                img_clip = ImageClip(item.img_path).set_start(start_time).set_duration(duration)
                
                # üëá G·ªåI H√ÄM ANIMATION (QUAN TR·ªåNG: X·ª¨ L√ù TUPLE TR·∫¢ V·ªÄ)
                # apply_animation b√¢y gi·ªù tr·∫£ v·ªÅ (clip_ƒë√£_x·ª≠_l√Ω, clip_c√°i_tay)
                result = apply_animation(
                    img_clip, 
                    item.anim_effect, 
                    info['x'], 
                    info['y'], 
                    SETTINGS['anim_duration'],
                    PATHS['hand_icon']
                )
                
                final_img_clip = None
                hand_clip = None

                # Ki·ªÉm tra xem k·∫øt qu·∫£ l√† tuple hay clip ƒë∆°n l·∫ª
                if isinstance(result, tuple):
                    final_img_clip, hand_clip = result
                else:
                    final_img_clip = result

                # Th√™m hi·ªáu ·ª©ng fade nh·∫π cho ·∫£nh ch√≠nh ƒë·ªÉ m∆∞·ª£t h∆°n
                if final_img_clip:
                    # L∆∞u √Ω: N·∫øu d√πng hi·ªáu ·ª©ng draw, fade ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b√™n trong animation.py r·ªìi
                    # n√™n ta ch·ªâ crossfadeout l√∫c bi·∫øn m·∫•t
                    final_img_clip = final_img_clip.crossfadeout(0.3)
                    image_layers.append(final_img_clip)

                # N·∫øu c√≥ clip c√°i tay (hi·ªáu ·ª©ng draw), th√™m v√†o layer tr√™n c√πng
                if hand_clip:
                    image_layers.append(hand_clip)

    # 3. X·ª¨ L√ù PH·ª§ ƒê·ªÄ
    subtitle_layers = []
    if os.path.exists(PATHS['sentences_srt_file']):
        subs_data = pysrt.open(PATHS['sentences_srt_file'], encoding='utf-8')
        for sub in subs_data:
            sub_clip = create_balanced_subtitle(
                sub.text, 
                sub.start.ordinal/1000.0, 
                sub.end.ordinal/1000.0, 
                video_size[0], 
                video_size[1]
            )
            subtitle_layers.append(sub_clip)

    # 4. T·ªîNG H·ª¢P FINAL
    bg_clip = ColorClip(size=video_size, color=SETTINGS['bg_color'], duration=total_duration)
    
    final_video = CompositeVideoClip(
        [bg_clip] + image_layers + subtitle_layers, 
        size=video_size
    ).set_audio(audio_clip)
    
    final_video = final_video.set_duration(audio_clip.duration)

    final_video.write_videofile(
        PATHS['output_video'], 
        fps=SETTINGS['video_fps'], 
        codec='libx264', 
        audio_codec='aac',
        ffmpeg_params=['-pix_fmt', 'yuv420p']
    )
    print(f"\nüéâ HO√ÄN TH√ÄNH: {PATHS['output_video']}")

if __name__ == "__main__":
    render_video()